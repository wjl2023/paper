# human-in-the-loop框架复杂背景抓取
human-in-the-loop框架，研究抓取过程中的语言界面，该界面允许human-intervention，基于state-of-the-art抓取基线。使用BERT将场景替换成文本表示。
## introduction
经典方法：将输入解析为场景图，以推断对象之间的空间关系并选择无碰撞的对象进行抓取。场景图中的节点表示对象，边表示对象之间的空间关系。与没有场景结构信息的端到端模型对比，这种方案能有效提高抓取的性能。
场景文本表示：用场景语言描述和视觉特征实现无冲突抓取。
场景的自然结构表示替代传统场景图结构，我们采用朱等人（2021）的模型，将场景图替换为要抓取的对象及其空间上下文的文本描述，并使用神经图像到文本模型进行场景理解，使用预训练语言模型表示场景文本，以便在后续抓取决策中进行视觉语言基础。
## 任务和配置
输入：相机图像；输出：抓取边界框。human-in-the-loop设置可以从场景理解模型（自我解释）或人类（人类干预）中获得。 
用堆叠关系标注对象->对象之间具备堆叠关系；添加全场景图来编码对象的成对关系；surface->指示对象是否位于顶部。
## 总体框架
将场景图分别输入基于卷积的抓取模型和场景理解模型。场景理解模型是一个图像到文本的组件，它生成一个描述要抓住的对象及其上下文的句子，通过预训练的语言模型和语言基础模型来选择与对象相关的抓取候选。当从图像到文本模型的描述不正确时，将给出来自人工干预的额外条件输入以校正场景描述。 
## 基于语言的抓取模型
联合概率集成额外的场景知识Kg作为辅助信息，以指导视觉的抓取。
### 抓取场景理解
基于图像到文本模型MMT来生成场景描述，这是一个基于编码器-解码器的转换器模型。该模型学习图像区域之间的关系的多级表示，并且在解码阶段使用网状连接来利用低和高级别特征。
训练中使用的适应：将MMT3中区域特征替换为区域特征和边界框特征的拼接；在训练期间增加一个额外的分数来乘以CIDEr-D奖励。
## 抓取的语言基础
使用视觉语言基础模型将场景描述映射到指定的对象。将任务部署在基于YOLOv3的单阶段语言基础模型上。形式上，场景图像I和场景描述Q分别被输入到视觉编码器和文本编码器，并且grounding模块输出具有编码器特征的grounding对象。文本编码器选择：BERT和Transformer。BERT是一个预先训练的语言模型，建立在Transformer网络上。描述被送入，导致768维tokens作为自然语言表示。
训练细节：端到端分为了图像到文本；语言基础；基于语言的抓取；
抓握检测描述：抓握建议；抓握方向分类和多抓握检测；抓握堆叠分类；
场景图生成模型：迭代消息传递模块（IMP）；关系图卷积网络（RGCN）；输入是来自使用场景图像I的公共对象检测模型的区域特征，IMP以三元组（即，<主语、谓语、对象>）的形式输出场景图Sg，并将其输入RGCN以预测每个节点（对象）的可抓取性。选择与边界框相对应的高置信度对象。